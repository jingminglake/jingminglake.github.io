---
layout:     post
title:      系统设计知识（二）
subtitle:   好友关系服务设计
date:       2021-03-08
author:     Jingming
header-img: img/post-bg-map.jpg
catalog: true
tags:
    - 系统设计
---

参考九章friendshipService设计。

### 数据库设计

好友关系分为单向和双向。

单向好友是微博的关注机制，双向好友是微信好友机制（不存在单向好友，一旦成为好友，必然是双向好友）。

单向好友，是指例如A关注了B。单向好友存储：表friendship，其中有外键(用户的id, 被关注者的id)

双向好友有两种存储方法，一个是只存一个pair(较小的id，较大的id)，这样查询的时候，就需要线性查，例如数据库存储（1，2），（2，3），当需要查出2的全部好友的时候，select的时候，不仅要看第一个值为2的pair，也要看pair的第二个值：把第二个值为2的pair选出来，然后在把其中第一个值取出来；或者两次查询，第一次查把pair的第一个值拿出来，第二次查把pair的第二个值拿出来。
另一个是存两份，这样存储空间消耗多，但是可以以第一个字段为key进行二分查找，所以更快。

PS. nosql存储的话只能拆成两条数据。

支持transaction的（也就是加锁机制）不能选nosql。

sql更成熟，以及帮你做了很多事，而nosql需要亲力亲为（序列化、第二索引）。硬盘型nosql效率比sql高10倍。
数据库索引：主键自带索引，如果对非主键属性进行索引，那就是建立一个额外的小表（看不见），里面对该属性排序，且指向到大表中对应的item地址。
index的坏处，太多index的话，插入修改效率降低，因为要修改index所对应的小表。
sql存储单位是行，nosql存储单位是格子（多了列id，剩余部分作为格子里的值，也就是以前的行，现在看作点），也就是三元组。

cassandra：

row_key是hash_key（hash后事无序的，因此不能做range query，也就是说例如查userId1到100的数据），决定分布式时候存放在的机器的位置；最常用的rowKey是userId，例如存newsfeed table，userId作为row_key。

column_key是排序的(partition key)，支持range query。注意在表中column_key不唯一，意思是，之前的若干记录行，现在在表中记录为（userId，tweetId1）和(userId, tweetId2)....若干点。另外，column_key可以是组合key，例如创建的时间结合tweetId做column_key，这样在创建的时间相同的时候，按第二个tweetId进行排序。

how to scale: (1) 防止单点失效(例如一台机器短暂的挂或者彻底的挂，导致网站不可用)，使用sharding，将数据拆分成不同部分，保存在不同机器上；replica是做数据备份（这同时也可以分摊读请求）。nosql自带sharding（sql不自带）。

垂直sharding：不同表放不同机器或一张表可以按列拆分成多个表并存于不同机器，例如一个表可以根据字段的更新频率切分成两个表（user profile table和user table，区别是user table比较核心，profile只是更多的介绍user的信息），垂直sharding优点是表有些分表部分挂了也没关系，坏处是有些表数据增加很多，垂直拆没有用。

水平sharding：按行拆分，按模固定数字（机器的数量）的方法，在新加机器的时候，由于之前mod数变化，之前数据放的位置不一样导致数据迁徙，成本太大，解决方法是一致性hash；按线性划分则不容易均匀切分，比如从3等份到4等份。

### 一致性哈希

之前每次加地址后，导致之前数据的模后地址变化了，大量数据需要迁移。

那么，为了减少迁移量：

1. 如何消除新增地址空间后hash变化？

答：为何不直接一开始就模一个足够大的固定的数，例如0-2^64-1。

2. 在1的解法基础上，如何均衡负载呢？

假设现在有n台机器，新增第n+1台机器。

首先，现在有n+1台机器，那么规定n+1个区间，每个区间的大小是2^64 / (n + 1)，也就是机器平分全部空间。

第n+1台机器负责的区间是mod后的数在所有mod后的数中的排序来的。那么，存在一个问题，例如3台机器平分1/3，

第四台机器来了，怎么向除了自己邻居的那台机器要走数据从而达到每个机器都分1/4？

解决方法是：每台机器负责的区间大小不变，但是拆分成不连续不等的若干个小区间， 例如拆分成1000个，并随机hash到空间（看作一个环）中。

每个点都负责接收最近的顺时针环上的数据（数据的hash值沿环顺时针走发现的第一个点）。该算法的均匀性取决于随机的均匀性。回到之前的问题，

前3台机器的3000个点平均的分散在环上，此时第四台机器过来了，那么第4台机器的1000个点，就会"均匀的"向之前的3台机器的3000个要数据了。

具体来说，会向顺时针最近虚拟节点把那一段（下一个节点的负责的起点到新加节点）数据要过来，可能有若干个新增节点需要向同一个  
虚拟节点要数据。

小结：hash到0-2^64-1环上，然后将每台实际机器M0看成是1000个虚拟结点(M0-0,M0-1,...)，hash到环上。数据存入顺时针最近的虚拟机器上。

新机器加入后做迁移：每个虚拟结点向顺时针的不是自己机器的第一个node要数据。

### 访问网址发生什么

浏览器端输入网址后：
1. 访问最近的DNS服务器得到域名对应的IP
2. 浏览器向该IP发送http请求
3. 服务器接收到请求，将请求交给80端口的程序（也就是http server，例如Apache）去处理
4. http server把请求给部署的web application来处理
5. web application会：（1）根据路径"/"后的部分找到对应的处理模块
                     （2）根据http是get还是post决定如何获取/存放数据
                     （3）从数据库服务中读取数据
                     （4）将结果组织成html网页
web application框架：Django、Ruby on Rails、NodeJS。
6. 浏览器获得网页
